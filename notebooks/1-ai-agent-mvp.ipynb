{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ› ï¸ Part 1: Building Your First Local AI Agent\n",
    "\n",
    "In this first section, we'll build a simple AI agent that runs entirely on your machine.  \n",
    "Instead of training a model from scratch or using complex frameworks, weâ€™ll use a **local LLM** (Gemma 3 4B or 12B) and interact with it directly through basic prompts.\n",
    "\n",
    "Along the way, weâ€™ll integrate:\n",
    "- **Gradio** to create a lightweight front end.\n",
    "- **SQLite** to log user inputs and model responses.\n",
    "- **Datasette** to easily explore logged conversations.\n",
    "\n",
    "By the end of this section, youâ€™ll have a working MVPâ€”a local chatbot with a UI, a database for observability, and a simple, reliable architecture you can build on.\n",
    "\n",
    "Later in the workshop, we'll build on these foundations:\n",
    "- We'll create a basic **agent** that can call functions dynamically.\n",
    "- We'll then set up a **Model-Callable Protocol (MCP)** client and server to expose tools flexibly to an LLM.\n",
    "- But first, weâ€™ll focus on getting **Gemma 3 models running locally** and understanding the building blocks of LLM-powered applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Gemma 3 and Ollama?\n",
    "\n",
    "### ğŸŒŸ Gemma 3: Efficient and Powerful LLMs for Local Use\n",
    "\n",
    "![Alt text](img/gemma3.png)\n",
    "\n",
    "[Gemma](https://ai.google.dev/gemma) is a family of open-weight models from Google DeepMind, designed for efficiency and strong reasoning capabilities.  \n",
    "\n",
    "For this workshop, we'll use **Gemma 3 4B** or **Gemma 3 12B**, depending on your hardware.\n",
    "\n",
    "Key features:\n",
    "- **Quantization Aware Training (QAT)** models: Deliver strong performance while requiring less memory, making local deployment practical.\n",
    "- **Optimized for local inference**: Designed to run well even without specialized cloud infrastructure.\n",
    "- **Strong structured prompting capabilities**: Ideal for building reliable LLM apps.\n",
    "- **Open weights and flexible licensing**: Easy to experiment and build without vendor lock-in.\n",
    "\n",
    "### ğŸš€ Ollama: A Game Changer for Local LLMs  \n",
    "\n",
    "![Alt text](img/ollama.svg)\n",
    "\n",
    "[Ollama](https://ollama.com/) makes running **LLMs locally** seamless, without complex setup.  \n",
    "- **Pre-configured model execution**: No need to manually set up dependencies.  \n",
    "- **Efficient GPU/CPU inference**: Optimized for running on local machines.  \n",
    "- **Fast iteration loop**: Load a model once, then run queries without excessive overhead.  \n",
    "\n",
    "Together, **Gemma 3 + Ollama** provides a fast, flexible foundation for building your first real LLM-powered applicationâ€”running 100% on your own machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly test that everything is working!\n",
    "\n",
    "Below, we'll:\n",
    "- Load the Gemma 3 4B QAT model with Ollama.\n",
    "- Send a simple prompt to the model.\n",
    "- Print the response.\n",
    "\n",
    "If this succeeds, you're ready to move on to building a full app!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, here are three interesting facts about the ocean:\\n\\n1. **The Ocean Floor is More Like a Continent:** The deep ocean floor is surprisingly mountainous, with ranges higher than the Himalayas! Scientists have discovered massive mountain ranges, valleys, and even volcanoes beneath the waves.  It's a vastly different landscape than we typically imagine when we think of the ocean.\\n\\n2. **Microbes Make Up a Huge Part of the Oceanâ€™s Life:**  Estimates suggest that microbes â€“ bacteria, archaea, and viruses â€“ make up *over 80%* of the oceanâ€™s biomass! They are the base of the food web, playing a crucial role in nutrient cycling and supporting nearly all other marine life.  Youâ€™re essentially swimming in a giant microbial community.\\n\\n3. **Thereâ€™s Enough Water in the Ocean to Cover the Earth Twice:** The sheer volume of water in the ocean is staggering. It contains about 97% of Earth's total water supply. Thatâ€™s a lot of liquid!\\n\\n\\n\\nDo you want to hear a few more ocean facts, or perhaps facts about a specific aspect of the ocean (like marine life, ocean currents, or ocean conservation)?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "# Use the Gemma 3 model\n",
    "model = 'gemma3:4b-it-qat'  # or 'gemma3:12b-it-qat' if you think your machine can handle it ;)\n",
    "\n",
    "def single_turn(prompt):\n",
    "    response: ChatResponse = chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"What are three interesting facts about the ocean?\"\n",
    "single_turn(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to send a basic prompt to the model, let's build a simple front end!\n",
    "\n",
    "We'll use **Gradio** to create an interactive chat interface where users can type questions and see model responses instantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Creating Our Gradio App  \n",
    "\n",
    "![Alt text](img/gradio.png)\n",
    "\n",
    "Before we dive into the code, let's talk about **Gradio**â€”one of the easiest ways to spin up interactive front-ends for AI applications.  \n",
    "\n",
    "ğŸš€ **Why Gradio?**  \n",
    "- **Super fast MVP development**: Build an interactive AI demo in just a few lines of code.  \n",
    "- **No frontend experience required**: Just define a Python function, and Gradio handles the UI.  \n",
    "- **Part of the ğŸ¤— Hugging Face ecosystem**: Seamlessly integrates with **models, Spaces, and APIs**.  \n",
    "- **Great for rapid prototyping**: Test AI models with real users before scaling up.  \n",
    "\n",
    "We'll use **Gradio** to build a lightweight app that lets users send prompts to a **Gemma 3 model** and receive responsesâ€”without needing a full web server setup.\n",
    "\n",
    "For instruction purposes, we've included the code below, but we'll be running our apps from the command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import gradio as gr\n",
    "import ollama\n",
    "\n",
    "model = 'gemma3:4b-it-qat'  # or 'gemma3:12b-it-qat' if you have enough memory\n",
    "\n",
    "def chat_with_model(prompt):\n",
    "    response = ollama.chat(model=model, messages=[{'role': 'user', 'content': prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_model,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Type your message here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Chat with Gemma 3\",\n",
    "    description=\"Enter a message and get a response from the Gemma 3 model.\",\n",
    ")\n",
    "\n",
    "iface.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ What's Happening in This Code?  \n",
    "\n",
    "- ğŸ”„ **Imports Gradio & Ollama** â€“ We bring in the tools we need to build the UI and interact with the model.  \n",
    "- ğŸ§  **Defines the model** â€“ We're using **Gemma 3** (`gemma3:4b-it-qat`) to power the chatbot.\n",
    "- ğŸ’¬ **Creates a function (`chat_with_model`)** â€“ Sends user input to the model via **Ollama** and returns a response.  \n",
    "- ğŸ¨ **Builds the Gradio UI (`iface`)** â€“  \n",
    "  - **ğŸ“© Input**: A text box for user messages.  \n",
    "  - **ğŸ–¥ï¸ Output**: The model's response.  \n",
    "  - **ğŸ­ Title & Description**: A simple interface for chatting with Gemma.  \n",
    "- ğŸš€ **Launches the app!** â€“ Runs the interactive chatbot in your browser.  \n",
    "\n",
    "Now, letâ€™s fire it up and start chatting! ğŸ”¥  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding observability with SQLite and Datasette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Why Tracing & Observability Matter\n",
    "\n",
    "Building an AI system isnâ€™t just about **getting a response**â€”itâ€™s about **understanding and improving how your model behaves over time**.  \n",
    "- **ğŸ‘€ Observability** helps us track inputs, outputs, and model decisions, making debugging and iteration easier.  \n",
    "- **ğŸ› Tracing conversations** lets us spot patterns, catch failure cases, and fine-tune our system for better performance.  \n",
    "- **ğŸ“ˆ Data-Driven Decisions**: Instead of guessing if the model is working well, we can use **real logged interactions** to refine prompts, improve accuracy, and compare models.  \n",
    "\n",
    "## ğŸ—„ï¸ Why SQLite? A No-Brainer for MVPs  \n",
    "\n",
    "![Alt text](img/sqlite.png)\n",
    "\n",
    "For **early-stage apps**, **SQLite** is an **ideal** choice for logging and observability:  \n",
    "- **ğŸ› ï¸ No Setup Hassle** â€“ Itâ€™s a self-contained, file-based database. No server required.  \n",
    "- **âš¡ Fast & Lightweight** â€“ Handles reads and writes efficiently without extra overhead.  \n",
    "- **ğŸ“¦ Portable & Easy to Share** â€“ Just a single file (`.db`) that works across different environments.  \n",
    "- **ğŸ”— Overwhelmingly Popular** â€“ Used in everything from **mobile apps (iOS, Android)** to **browsers (Chrome, Firefox)** and even **airplane black boxes**!  \n",
    "\n",
    "### ğŸš€ Future Scaling  \n",
    "Right now, **SQLite is perfect** for logging and inspecting model interactions. Later, if we move to **multi-user or production-scale apps**, we can switch to **PostgreSQL, MySQL, or cloud-based solutions**â€”but for now, SQLite keeps things simple and effective.  \n",
    "\n",
    "---\n",
    "\n",
    "Next, weâ€™ll **log our prompts and responses** so we can start analyzing how our system is performing! ğŸ”\n",
    "\n",
    " As above, we've included the code below, although we'll be running our apps from the command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import gradio as gr\n",
    "import ollama\n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "# SQLite Database Setup\n",
    "DB_PATH = \"chat_log.db\"\n",
    "\n",
    "def setup_database():\n",
    "    \"\"\"Create a simple SQLite table if it doesn't exist.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS chat_history (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            prompt TEXT,\n",
    "            response TEXT,\n",
    "            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "setup_database()  # Ensure the DB is set up before running the app\n",
    "\n",
    "def chat_with_model(prompt):\n",
    "    \"\"\"Send user input to Ollama, get response, and log to SQLite.\"\"\"\n",
    "    response = ollama.chat(model=\"gemma3:4b-it-qat\", messages=[{\"role\": \"user\", \"content\": prompt}])[\"message\"][\"content\"]\n",
    "    \n",
    "    # Log the interaction to SQLite\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"INSERT INTO chat_history (prompt, response) VALUES (?, ?)\", (prompt, response))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    return response\n",
    "\n",
    "# Gradio UI\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_model,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Type your message here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Chat with Gemma\",\n",
    "    description=\"Enter a message and get a response from the Gemma 2B model. Your chats are logged in SQLite.\",\n",
    ")\n",
    "\n",
    "iface.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ What's Happening in This Code?  \n",
    "\n",
    "- ğŸ“¦ **Imports required libraries** â€“  \n",
    "  - `gradio` for the UI  \n",
    "  - `ollama` for running **Gemma 3**  \n",
    "  - `sqlite3` for logging interactions  \n",
    "  - `datetime` to track timestamps  \n",
    "\n",
    "- ğŸ—„ï¸ **Sets up a SQLite database (`chat_log.db`)** â€“  \n",
    "  - Creates a **`chat_history`** table (if it doesnâ€™t exist)  \n",
    "  - Stores **`prompt`**, **`response`**, and **timestamp** for each chat  \n",
    "\n",
    "- ğŸ’¬ **Defines `chat_with_model(prompt)`** â€“  \n",
    "  - Sends user input to **Ollama (Gemma 2B)**  \n",
    "  - Logs the chat to **SQLite**  \n",
    "  - Returns the modelâ€™s response  \n",
    "\n",
    "- ğŸ¨ **Creates a Gradio UI (`iface`)** â€“  \n",
    "  - **ğŸ“ Input:** A text box for user queries  \n",
    "  - **ğŸ–¥ï¸ Output:** The modelâ€™s response  \n",
    "  - **ğŸ“œ Description:** Informs users that chats are logged  \n",
    "\n",
    "- ğŸš€ **Launches the app!** â€“ Runs a browser-based chatbot with full logging  \n",
    "\n",
    "This setup ensures we can **track every interaction**, making debugging, evaluation, and iteration much easier. Next, let's test it out! ğŸ”  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ğŸ” Exploring Your Data with Datasette  \n",
    "\n",
    "![Alt text](img/datasette.png)\n",
    "\n",
    "Once weâ€™ve logged conversations in **SQLite**, we need an easy way to inspect and analyze them.  \n",
    "Thatâ€™s where **[Datasette](https://datasette.io/)** comes inâ€”a powerful tool for **browsing, querying, and exporting SQLite databases** effortlessly.  \n",
    "\n",
    "### ğŸš€ Why Datasette?  \n",
    "- **Instant Database UI** â€“ No SQL knowledge required; just open a browser and explore!  \n",
    "- **Lightning Fast** â€“ Designed for large-scale data publishing but perfect for small logs too.  \n",
    "- **Built-in Querying** â€“ Filter, sort, and search directly in a web-based UI.  \n",
    "- **Easy Exporting** â€“ Convert your database into **CSV**, **JSON**, or other formats with a click.  \n",
    "\n",
    "### ğŸ“¤ Exporting Traces to CSV  \n",
    "\n",
    "Weâ€™ll use **Datasette** to **export chat logs to a CSV file**, making it easier to analyze failure cases and refine our AI system.  \n",
    "This CSV can be used for:  \n",
    "- **ğŸ“Š Failure Mode Analysis** â€“ Identify common mistakes by reviewing responses.  \n",
    "- **ğŸ‘¥ Sharing with Subject Matter Experts** â€“ Non-technical teammates can review and give feedback.  \n",
    "- **âœ… Manual Evaluation** â€“ Open in a spreadsheet and score outputs with ğŸ‘/ğŸ‘ + comments.  \n",
    "- **ğŸ“ˆ Starting Systematic Evaluations** â€“ Lay the groundwork for automated performance tracking.  \n",
    "\n",
    "---\n",
    "\n",
    "Next, letâ€™s load up **Datasette**, explore our logged chats, and **export them for further analysis!** ğŸ§ğŸ“Š  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Recap: What We Learned  \n",
    "\n",
    "In this notebook, we focused on building the **first version of an LLM-powered classification system** that runs **entirely locally**. Hereâ€™s what we covered:  \n",
    "\n",
    "### ğŸ—ï¸ **Building an MVP AI System**  \n",
    "- Used **Gemma 3** + **Ollama** to create a **basic local chat app**.  \n",
    "- Built an interactive **Gradio** UI to test our system.  \n",
    "\n",
    "### ğŸ” **Logging & Observability**  \n",
    "- Stored model interactions in an **SQLite** database for **tracing and debugging**.  \n",
    "- Used **Datasette** to **browse logged conversations and export data**.  \n",
    "\n",
    "### ğŸ“¤ **Exporting for Further Exploration**  \n",
    "- Learned how to **export chat logs to CSV** for potential later analysis.  \n",
    "- Discussed how **structured logs** help track model responses over time.  \n",
    "\n",
    "### ğŸš€ **Why This Matters**  \n",
    "- **AI systems are more than just modelsâ€”they need observability and traceability.**  \n",
    "- **Logging interactions** makes debugging, iteration, and improvement possible.  \n",
    "- This lays the **foundation** for deeper **evaluation techniques** in upcoming sections.  \n",
    "\n",
    "In the next notebook, we'll take things further by **building a basic agent**â€”using function calling to let the model choose when and how to call external tools.\n",
    "We'll also start exploring new patterns like **multi-step reasoning** and **tool use** with local models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
