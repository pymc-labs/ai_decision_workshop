{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🛠️ Part 1: Building Your First Local AI Agent\n",
    "\n",
    "In this first section, we'll build a simple AI agent that runs entirely on your machine.  \n",
    "Instead of training a model from scratch or using complex frameworks, we’ll use a **local LLM** (Gemma 3 4B or 12B) and interact with it directly through basic prompts.\n",
    "\n",
    "Along the way, we’ll integrate:\n",
    "- **Gradio** to create a lightweight front end.\n",
    "- **SQLite** to log user inputs and model responses.\n",
    "- **Datasette** to easily explore logged conversations.\n",
    "\n",
    "By the end of this section, you’ll have a working MVP—a local chatbot with a UI, a database for observability, and a simple, reliable architecture you can build on.\n",
    "\n",
    "Later in the workshop, we'll build on these foundations:\n",
    "- We'll create a basic **agent** that can call functions dynamically.\n",
    "- We'll then set up a **Model-Callable Protocol (MCP)** client and server to expose tools flexibly to an LLM.\n",
    "- But first, we’ll focus on getting **Gemma 3 models running locally** and understanding the building blocks of LLM-powered applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Gemma 3 and Ollama?\n",
    "\n",
    "### 🌟 Gemma 3: Efficient and Powerful LLMs for Local Use\n",
    "\n",
    "![Alt text](img/gemma3.png)\n",
    "\n",
    "[Gemma](https://ai.google.dev/gemma) is a family of open-weight models from Google DeepMind, designed for efficiency and strong reasoning capabilities.  \n",
    "\n",
    "For this workshop, we'll use **Gemma 3 4B** or **Gemma 3 12B**, depending on your hardware.\n",
    "\n",
    "Key features:\n",
    "- **Quantization Aware Training (QAT)** models: Deliver strong performance while requiring less memory, making local deployment practical.\n",
    "- **Optimized for local inference**: Designed to run well even without specialized cloud infrastructure.\n",
    "- **Strong structured prompting capabilities**: Ideal for building reliable LLM apps.\n",
    "- **Open weights and flexible licensing**: Easy to experiment and build without vendor lock-in.\n",
    "\n",
    "### 🚀 Ollama: A Game Changer for Local LLMs  \n",
    "\n",
    "![Alt text](img/ollama.svg)\n",
    "\n",
    "[Ollama](https://ollama.com/) makes running **LLMs locally** seamless, without complex setup.  \n",
    "- **Pre-configured model execution**: No need to manually set up dependencies.  \n",
    "- **Efficient GPU/CPU inference**: Optimized for running on local machines.  \n",
    "- **Fast iteration loop**: Load a model once, then run queries without excessive overhead.  \n",
    "\n",
    "Together, **Gemma 3 + Ollama** provides a fast, flexible foundation for building your first real LLM-powered application—running 100% on your own machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly test that everything is working!\n",
    "\n",
    "Below, we'll:\n",
    "- Load the Gemma 3 4B QAT model with Ollama.\n",
    "- Send a simple prompt to the model.\n",
    "- Print the response.\n",
    "\n",
    "If this succeeds, you're ready to move on to building a full app!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, here are three interesting facts about the ocean:\\n\\n1. **The Ocean Floor is More Like a Continent:** The deep ocean floor is surprisingly mountainous, with ranges higher than the Himalayas! Scientists have discovered massive mountain ranges, valleys, and even volcanoes beneath the waves.  It's a vastly different landscape than we typically imagine when we think of the ocean.\\n\\n2. **Microbes Make Up a Huge Part of the Ocean’s Life:**  Estimates suggest that microbes – bacteria, archaea, and viruses – make up *over 80%* of the ocean’s biomass! They are the base of the food web, playing a crucial role in nutrient cycling and supporting nearly all other marine life.  You’re essentially swimming in a giant microbial community.\\n\\n3. **There’s Enough Water in the Ocean to Cover the Earth Twice:** The sheer volume of water in the ocean is staggering. It contains about 97% of Earth's total water supply. That’s a lot of liquid!\\n\\n\\n\\nDo you want to hear a few more ocean facts, or perhaps facts about a specific aspect of the ocean (like marine life, ocean currents, or ocean conservation)?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "# Use the Gemma 3 model\n",
    "model = 'gemma3:4b-it-qat'  # or 'gemma3:12b-it-qat' if you think your machine can handle it ;)\n",
    "\n",
    "def single_turn(prompt):\n",
    "    response: ChatResponse = chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"What are three interesting facts about the ocean?\"\n",
    "single_turn(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to send a basic prompt to the model, let's build a simple front end!\n",
    "\n",
    "We'll use **Gradio** to create an interactive chat interface where users can type questions and see model responses instantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Creating Our Gradio App  \n",
    "\n",
    "![Alt text](img/gradio.png)\n",
    "\n",
    "Before we dive into the code, let's talk about **Gradio**—one of the easiest ways to spin up interactive front-ends for AI applications.  \n",
    "\n",
    "🚀 **Why Gradio?**  \n",
    "- **Super fast MVP development**: Build an interactive AI demo in just a few lines of code.  \n",
    "- **No frontend experience required**: Just define a Python function, and Gradio handles the UI.  \n",
    "- **Part of the 🤗 Hugging Face ecosystem**: Seamlessly integrates with **models, Spaces, and APIs**.  \n",
    "- **Great for rapid prototyping**: Test AI models with real users before scaling up.  \n",
    "\n",
    "We'll use **Gradio** to build a lightweight app that lets users send prompts to a **Gemma 3 model** and receive responses—without needing a full web server setup.\n",
    "\n",
    "For instruction purposes, we've included the code below, but we'll be running our apps from the command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import gradio as gr\n",
    "import ollama\n",
    "\n",
    "model = 'gemma3:4b-it-qat'  # or 'gemma3:12b-it-qat' if you have enough memory\n",
    "\n",
    "def chat_with_model(prompt):\n",
    "    response = ollama.chat(model=model, messages=[{'role': 'user', 'content': prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_model,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Type your message here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Chat with Gemma 3\",\n",
    "    description=\"Enter a message and get a response from the Gemma 3 model.\",\n",
    ")\n",
    "\n",
    "iface.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 What's Happening in This Code?  \n",
    "\n",
    "- 🔄 **Imports Gradio & Ollama** – We bring in the tools we need to build the UI and interact with the model.  \n",
    "- 🧠 **Defines the model** – We're using **Gemma 3** (`gemma3:4b-it-qat`) to power the chatbot.\n",
    "- 💬 **Creates a function (`chat_with_model`)** – Sends user input to the model via **Ollama** and returns a response.  \n",
    "- 🎨 **Builds the Gradio UI (`iface`)** –  \n",
    "  - **📩 Input**: A text box for user messages.  \n",
    "  - **🖥️ Output**: The model's response.  \n",
    "  - **🎭 Title & Description**: A simple interface for chatting with Gemma.  \n",
    "- 🚀 **Launches the app!** – Runs the interactive chatbot in your browser.  \n",
    "\n",
    "Now, let’s fire it up and start chatting! 🔥  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding observability with SQLite and Datasette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Why Tracing & Observability Matter\n",
    "\n",
    "Building an AI system isn’t just about **getting a response**—it’s about **understanding and improving how your model behaves over time**.  \n",
    "- **👀 Observability** helps us track inputs, outputs, and model decisions, making debugging and iteration easier.  \n",
    "- **🐛 Tracing conversations** lets us spot patterns, catch failure cases, and fine-tune our system for better performance.  \n",
    "- **📈 Data-Driven Decisions**: Instead of guessing if the model is working well, we can use **real logged interactions** to refine prompts, improve accuracy, and compare models.  \n",
    "\n",
    "## 🗄️ Why SQLite? A No-Brainer for MVPs  \n",
    "\n",
    "![Alt text](img/sqlite.png)\n",
    "\n",
    "For **early-stage apps**, **SQLite** is an **ideal** choice for logging and observability:  \n",
    "- **🛠️ No Setup Hassle** – It’s a self-contained, file-based database. No server required.  \n",
    "- **⚡ Fast & Lightweight** – Handles reads and writes efficiently without extra overhead.  \n",
    "- **📦 Portable & Easy to Share** – Just a single file (`.db`) that works across different environments.  \n",
    "- **🔗 Overwhelmingly Popular** – Used in everything from **mobile apps (iOS, Android)** to **browsers (Chrome, Firefox)** and even **airplane black boxes**!  \n",
    "\n",
    "### 🚀 Future Scaling  \n",
    "Right now, **SQLite is perfect** for logging and inspecting model interactions. Later, if we move to **multi-user or production-scale apps**, we can switch to **PostgreSQL, MySQL, or cloud-based solutions**—but for now, SQLite keeps things simple and effective.  \n",
    "\n",
    "---\n",
    "\n",
    "Next, we’ll **log our prompts and responses** so we can start analyzing how our system is performing! 🔍\n",
    "\n",
    " As above, we've included the code below, although we'll be running our apps from the command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import gradio as gr\n",
    "import ollama\n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "# SQLite Database Setup\n",
    "DB_PATH = \"chat_log.db\"\n",
    "\n",
    "def setup_database():\n",
    "    \"\"\"Create a simple SQLite table if it doesn't exist.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS chat_history (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            prompt TEXT,\n",
    "            response TEXT,\n",
    "            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "setup_database()  # Ensure the DB is set up before running the app\n",
    "\n",
    "def chat_with_model(prompt):\n",
    "    \"\"\"Send user input to Ollama, get response, and log to SQLite.\"\"\"\n",
    "    response = ollama.chat(model=\"gemma3:4b-it-qat\", messages=[{\"role\": \"user\", \"content\": prompt}])[\"message\"][\"content\"]\n",
    "    \n",
    "    # Log the interaction to SQLite\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"INSERT INTO chat_history (prompt, response) VALUES (?, ?)\", (prompt, response))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    return response\n",
    "\n",
    "# Gradio UI\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_model,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Type your message here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Chat with Gemma\",\n",
    "    description=\"Enter a message and get a response from the Gemma 2B model. Your chats are logged in SQLite.\",\n",
    ")\n",
    "\n",
    "iface.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 What's Happening in This Code?  \n",
    "\n",
    "- 📦 **Imports required libraries** –  \n",
    "  - `gradio` for the UI  \n",
    "  - `ollama` for running **Gemma 3**  \n",
    "  - `sqlite3` for logging interactions  \n",
    "  - `datetime` to track timestamps  \n",
    "\n",
    "- 🗄️ **Sets up a SQLite database (`chat_log.db`)** –  \n",
    "  - Creates a **`chat_history`** table (if it doesn’t exist)  \n",
    "  - Stores **`prompt`**, **`response`**, and **timestamp** for each chat  \n",
    "\n",
    "- 💬 **Defines `chat_with_model(prompt)`** –  \n",
    "  - Sends user input to **Ollama (Gemma 2B)**  \n",
    "  - Logs the chat to **SQLite**  \n",
    "  - Returns the model’s response  \n",
    "\n",
    "- 🎨 **Creates a Gradio UI (`iface`)** –  \n",
    "  - **📝 Input:** A text box for user queries  \n",
    "  - **🖥️ Output:** The model’s response  \n",
    "  - **📜 Description:** Informs users that chats are logged  \n",
    "\n",
    "- 🚀 **Launches the app!** – Runs a browser-based chatbot with full logging  \n",
    "\n",
    "This setup ensures we can **track every interaction**, making debugging, evaluation, and iteration much easier. Next, let's test it out! 🔍  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 🔍 Exploring Your Data with Datasette  \n",
    "\n",
    "![Alt text](img/datasette.png)\n",
    "\n",
    "Once we’ve logged conversations in **SQLite**, we need an easy way to inspect and analyze them.  \n",
    "That’s where **[Datasette](https://datasette.io/)** comes in—a powerful tool for **browsing, querying, and exporting SQLite databases** effortlessly.  \n",
    "\n",
    "### 🚀 Why Datasette?  \n",
    "- **Instant Database UI** – No SQL knowledge required; just open a browser and explore!  \n",
    "- **Lightning Fast** – Designed for large-scale data publishing but perfect for small logs too.  \n",
    "- **Built-in Querying** – Filter, sort, and search directly in a web-based UI.  \n",
    "- **Easy Exporting** – Convert your database into **CSV**, **JSON**, or other formats with a click.  \n",
    "\n",
    "### 📤 Exporting Traces to CSV  \n",
    "\n",
    "We’ll use **Datasette** to **export chat logs to a CSV file**, making it easier to analyze failure cases and refine our AI system.  \n",
    "This CSV can be used for:  \n",
    "- **📊 Failure Mode Analysis** – Identify common mistakes by reviewing responses.  \n",
    "- **👥 Sharing with Subject Matter Experts** – Non-technical teammates can review and give feedback.  \n",
    "- **✅ Manual Evaluation** – Open in a spreadsheet and score outputs with 👍/👎 + comments.  \n",
    "- **📈 Starting Systematic Evaluations** – Lay the groundwork for automated performance tracking.  \n",
    "\n",
    "---\n",
    "\n",
    "Next, let’s load up **Datasette**, explore our logged chats, and **export them for further analysis!** 🧐📊  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Recap: What We Learned  \n",
    "\n",
    "In this notebook, we focused on building the **first version of an LLM-powered classification system** that runs **entirely locally**. Here’s what we covered:  \n",
    "\n",
    "### 🏗️ **Building an MVP AI System**  \n",
    "- Used **Gemma 3** + **Ollama** to create a **basic local chat app**.  \n",
    "- Built an interactive **Gradio** UI to test our system.  \n",
    "\n",
    "### 🔍 **Logging & Observability**  \n",
    "- Stored model interactions in an **SQLite** database for **tracing and debugging**.  \n",
    "- Used **Datasette** to **browse logged conversations and export data**.  \n",
    "\n",
    "### 📤 **Exporting for Further Exploration**  \n",
    "- Learned how to **export chat logs to CSV** for potential later analysis.  \n",
    "- Discussed how **structured logs** help track model responses over time.  \n",
    "\n",
    "### 🚀 **Why This Matters**  \n",
    "- **AI systems are more than just models—they need observability and traceability.**  \n",
    "- **Logging interactions** makes debugging, iteration, and improvement possible.  \n",
    "- This lays the **foundation** for deeper **evaluation techniques** in upcoming sections.  \n",
    "\n",
    "In the next notebook, we'll take things further by **building a basic agent**—using function calling to let the model choose when and how to call external tools.\n",
    "We'll also start exploring new patterns like **multi-step reasoning** and **tool use** with local models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
