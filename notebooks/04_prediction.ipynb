{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Prediction\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pymc-labs/ai_decision_workshop/blob/main/notebooks/04_prediction.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 75\n",
    "plt.rcParams['figure.figsize'] = (6.4, 3.2)\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions of Probabilities\n",
    "\n",
    "Suppose we have two coins:\n",
    "\n",
    "* One is known to be fair, so the probability of heads is 50%.\n",
    "\n",
    "* The other is known to be biased, but it is equally likely that the probability of heads is 30% or 70%.\n",
    "\n",
    "If we flip each coin only once, the probability of heads is 50% for both coins.\n",
    "So it is tempting to think there is no difference: the outcome depends only on the mean probability.\n",
    "\n",
    "But that's not true in general. For example, suppose we toss each coin 10 times.\n",
    "For the fair coin, the distribution of outcomes is a simple binomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "n = 10\n",
    "binomial_dist = binom(n=n, p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compute its PMF like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(n+1)\n",
    "pmf_fair = binomial_dist.pmf(ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the biased coin, there are two possible distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_dist1 = binom(n=n, p=0.3)\n",
    "binomial_dist2 = binom(n=n, p=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compute the overall PMF by averaging them (in this case because 30% and 70% are equally likely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmf1 = binomial_dist1.pmf(ks)\n",
    "pmf2 = binomial_dist2.pmf(ks)\n",
    "pmf_biased = (pmf1 + pmf2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a mixture of two binomials.\n",
    "\n",
    "Here's what the two predictive distributions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 0.45\n",
    "plt.bar(ks, pmf_fair, label='fair', align=\"edge\", width=-width)\n",
    "plt.bar(ks, pmf_biased, label='biased', align=\"edge\", width=width)\n",
    "plt.xlabel('Number of heads')\n",
    "plt.ylabel('PMF')\n",
    "plt.title('Predictive distributions')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are substantially different! In particular, the biased coin is more likely to produce extreme results.\n",
    "\n",
    "In general, if we have a distribution of probabilities, the predictive distribution is a mixture of binomials.\n",
    "\n",
    "In the philosophy of probability, so-called \"second order probabilities\" have sometimes been considered problematic.\n",
    "In the Bayesian interpretation of probability, they are unproblematically meaningful and practically useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Disaster\n",
    "\n",
    "Suppose you are designing a safety critical system, like a nuclear power plant, a medical device, or an autonomous vehicle.\n",
    "You have identified five components of the system that might fail, and estimated that there is a 5% chance that any one of them fails during a particular period of time.\n",
    "Fortunately, these components are redundant, so the system only fails if *all five of the components fail*.\n",
    "\n",
    "If the probability of failure is known to be precisely 5%, the probability of five simultaneous failures is 0.05 raised to the fifth power, which is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.05 ** 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's see what happens if we add some uncertainty to that estimate.\n",
    "We'll use a beta distribution to represent uncertainty about the probability of failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "mean = 0.05\n",
    "std = 0.001\n",
    "\n",
    "alpha = mean * ((mean * (1 - mean)) / std**2 - 1)\n",
    "beta = (1 - mean) * ((mean * (1 - mean)) / std**2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = beta_dist(alpha, beta)\n",
    "print(dist.mean(), dist.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what that distribution looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dist.rvs(1000)\n",
    "az.plot_kde(sample)\n",
    "plt.xlabel('Probability of component failure')\n",
    "plt.ylabel('Density');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the probability of **system failure**, we'll draw 1000 values from the beta distribution and compute the fifth power of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dist.rvs(1000)**5\n",
    "print(sample.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the distribution looks like for the probability of system failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_kde(sample)\n",
    "plt.xlabel('Probability of system failure')\n",
    "plt.ylabel('Density');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the uncertainty is small, the probability of system failure is still small.\n",
    "\n",
    "Exercise: Go back and increase `std`. The mean probability of component failure should stay the same, but see what happens to the mean probability of system failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The way we are modeling this scenario, we assume that the probability of failure is the same for all five components. But the same effect happens if they are not identical, but correlated -- that is, if one of them turns out to be higher than expected, it's more likely that they others are, too. If they are uncorrelated, or only weakly correlated, we don't have the same problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
